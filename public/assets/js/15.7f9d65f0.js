(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{526:function(a,s,e){"use strict";e.r(s);var n=e(6),r=Object(n.a)({},(function(){var a=this,s=a.$createElement,e=a._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"配置集群-分布式环境"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#配置集群-分布式环境"}},[a._v("#")]),a._v(" 配置集群/分布式环境")]),a._v(" "),e("p",[a._v("在配置集群/分布式模式时，需要修改“/usr/local/hadoop/etc/hadoop”目录下的配置文件，这里仅设置正常启动所必须的设置项，包括workers 、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件，更多设置项可查看官方说明。")]),a._v(" "),e("h2",{attrs:{id:"_1-修改文件workers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-修改文件workers"}},[a._v("#")]),a._v(" （1）修改文件workers")]),a._v(" "),e("p",[a._v("需要把所有数据节点的主机名写入该文件，每行一个，默认为 localhost（即把本机作为数据节点），所以，在伪分布式配置时，就采用了这种默认的配置，使得节点既作为名称节点也作为数据节点。在进行分布式配置时，可以保留localhost，让Master节点同时充当名称节点和数据节点，或者也可以删掉localhost这行，让Master节点仅作为名称节点使用。\n本教程让Master节点仅作为名称节点使用，因此将workers文件中原来的localhost删除，只添加如下一行内容：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("Slave1\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("h2",{attrs:{id:"_2-修改文件core-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改文件core-site-xml"}},[a._v("#")]),a._v(" （2）修改文件core-site.xml")]),a._v(" "),e("p",[a._v("请把core-site.xml文件修改为如下内容：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("<configuration>\n        <property>\n                <name>fs.defaultFS</name>\n                <value>hdfs://Master:9000</value>\n        </property>\n        <property>\n                <name>hadoop.tmp.dir</name>\n                <value>file:/usr/local/hadoop/tmp</value>\n                <description>Abase for other temporary directories.</description>\n        </property>\n</configuration>\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br")])]),e("p",[a._v("各个配置项的含义可以参考前面伪分布式模式时的介绍，这里不再赘述。")]),a._v(" "),e("h2",{attrs:{id:"_3-修改文件hdfs-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-修改文件hdfs-site-xml"}},[a._v("#")]),a._v(" （3）修改文件hdfs-site.xml")]),a._v(" "),e("p",[a._v("对于Hadoop的分布式文件系统HDFS而言，一般都是采用冗余存储，冗余因子通常为3，也就是说，一份数据保存三份副本。但是，本教程只有一个Slave节点作为数据节点，即集群中只有一个数据节点，数据只能保存一份，所以 ，dfs.replication的值还是设置为 1。hdfs-site.xml具体内容如下：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("<configuration>\n        <property>\n                <name>dfs.namenode.secondary.http-address</name>\n                <value>Master:50090</value>\n        </property>\n        <property>\n                <name>dfs.replication</name>\n                <value>1</value>\n        </property>\n        <property>\n                <name>dfs.namenode.name.dir</name>\n                <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n        </property>\n        <property>\n                <name>dfs.datanode.data.dir</name>\n                <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n        </property>\n</configuration>\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br")])]),e("h2",{attrs:{id:"_4-修改文件mapred-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-修改文件mapred-site-xml"}},[a._v("#")]),a._v(" （4）修改文件mapred-site.xml")]),a._v(" "),e("p",[a._v("“/usr/local/hadoop/etc/hadoop”目录下有一个mapred-site.xml.template，需要修改文件名称，把它重命名为mapred-site.xml，然后，把mapred-site.xml文件配置成如下内容：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("<configuration>\n        <property>\n                <name>mapreduce.framework.name</name>\n                <value>yarn</value>\n        </property>\n        <property>\n                <name>mapreduce.jobhistory.address</name>\n                <value>Master:10020</value>\n        </property>\n        <property>\n                <name>mapreduce.jobhistory.webapp.address</name>\n                <value>Master:19888</value>\n        </property>\n        <property>\n                <name>yarn.app.mapreduce.am.env</name>\n                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>\n        </property>\n        <property>\n                <name>mapreduce.map.env</name>\n                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>\n        </property>\n        <property>\n                <name>mapreduce.reduce.env</name>\n                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>\n        </property> \n</configuration>\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br")])]),e("h2",{attrs:{id:"_5-修改文件-yarn-site-xml"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-修改文件-yarn-site-xml"}},[a._v("#")]),a._v(" （5）修改文件 yarn-site.xml")]),a._v(" "),e("p",[a._v("请把yarn-site.xml文件配置成如下内容：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("<configuration>\n        <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>Master</value>\n        </property>\n        <property>\n                <name>yarn.nodemanager.aux-services</name>\n                <value>mapreduce_shuffle</value>\n        </property>\n</configuration>\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br")])]),e("p",[a._v("上述5个文件全部配置完成以后，需要把Master节点上的“/usr/local/hadoop”文件夹复制到各个节点上。如果之前已经运行过伪分布式模式，建议在切换到集群模式之前首先删除之前在伪分布式模式下生成的临时文件。具体来说，需要首先在Master节点上执行如下命令：")]),a._v(" "),e("div",{staticClass:"language-bash line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/localsudo "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("rm")]),a._v(" -r ./hadoop/tmp     "),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 删除 Hadoop 临时文件sudo rm -r ./hadoop/logs/*   # 删除日志文件tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制cd ~scp ./hadoop.master.tar.gz Slave1:/home/hadoop")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("Shell 命令")]),a._v(" "),e("p",[a._v("然后在Slave1节点上执行如下命令：")]),a._v(" "),e("div",{staticClass:"language-bash line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token function"}},[a._v("sudo")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("rm")]),a._v(" -r /usr/local/hadoop    "),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 删掉旧的（如果存在）sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/localsudo chown -R hadoop /usr/local/hadoop")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("Shell 命令")]),a._v(" "),e("p",[a._v("同样，如果有其他Slave节点，也要执行将hadoop.master.tar.gz传输到Slave节点以及在Slave节点解压文件的操作。\n首次启动Hadoop集群时，需要先在Master节点执行名称节点的格式化（只需要执行这一次，后面再启动Hadoop时，不要再次格式化名称节点），命令如下：")]),a._v(" "),e("div",{staticClass:"language-bash line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[a._v("hdfs namenode -format\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("Shell 命令")]),a._v(" "),e("p",[a._v("现在就可以启动Hadoop了，启动需要在Master节点上进行，执行如下命令：")]),a._v(" "),e("div",{staticClass:"language-bash line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[a._v("start-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserver\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("Shell 命令")]),a._v(" "),e("p",[a._v("通过命令jps可以查看各个节点所启动的进程。如果已经正确启动，则在Master节点上可以看到NameNode、ResourceManager、SecondrryNameNode和JobHistoryServer进程，如下图所示。\n"),e("img",{attrs:{src:"http://dblab.xmu.edu.cn/blog/wp-content/uploads/2020/07/hadoop-distributed-install06.png",alt:"img"}})]),a._v(" "),e("p",[a._v("在Slave节点可以看到DataNode和NodeManager进程，如下图所示。\n"),e("img",{attrs:{src:"http://dblab.xmu.edu.cn/blog/wp-content/uploads/2020/07/hadoop-distributed-install07.png",alt:"img"}})]),a._v(" "),e("p",[a._v("缺少任一进程都表示出错。另外还需要在Master节点上通过命令“hdfs dfsadmin -report”查看数据节点是否正常启动，如果屏幕信息中的“Live datanodes”不为 0 ，则说明集群启动成功。由于本教程只有1个Slave节点充当数据节点，因此，数据节点启动成功以后，会显示如下图所示信息。\n"),e("img",{attrs:{src:"http://dblab.xmu.edu.cn/blog/wp-content/uploads/2020/07/hadoop-distributed-install08.png",alt:"img"}})]),a._v(" "),e("p",[a._v("也可以在Linux系统的浏览器中输入地址“http://master:9870/”，通过 Web 页面看到查看名称节点和数据节点的状态。如果不成功，可以通过启动日志排查原因。\n这里再次强调，伪分布式模式和分布式模式切换时需要注意以下事项：\n（a）从分布式切换到伪分布式时，不要忘记修改slaves配置文件；\n（b）在两者之间切换时，若遇到无法正常启动的情况，可以删除所涉及节点的临时文件夹，这样虽然之前的数据会被删掉，但能保证集群正确启动。所以，如果集群以前能启动，但后来启动不了，特别是数据节点无法启动，不妨试着删除所有节点（包括Slave节点）上的“/usr/local/hadoop/tmp”文件夹，再重新执行一次“hdfs namenode -format”，再次启动即可。")])])}),[],!1,null,null,null);s.default=r.exports}}]);